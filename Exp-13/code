import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_absolute_error, r2_score

# Load the dataset
data = pd.read_csv("CarPrice.csv")

# Display basic info
print(data.head())
print(f"Dataset shape: {data.shape}")
print(f"Missing values:\n{data.isnull().sum()}")
print(data.info())

# Drop non-numeric columns before correlation analysis
numeric_data = data.select_dtypes(include=[np.number])
print("\nCorrelation Matrix:\n", numeric_data.corr())

# Heatmap of correlations
plt.figure(figsize=(20, 15))
sns.heatmap(numeric_data.corr(), cmap="coolwarm", annot=True)
plt.title("Feature Correlation Heatmap")
plt.show()

# Encode categorical variables using one-hot encoding
data = pd.get_dummies(data, columns=["CarName"], drop_first=True)

# Select relevant numeric features for model training
features = ["symboling", "wheelbase", "carlength", "carwidth", "carheight",
            "curbweight", "enginesize", "boreratio", "stroke", 
            "compressionratio", "horsepower", "peakrpm", "citympg", "highwaympg"]

target = "price"

# Drop rows with missing values (if any)
data = data.dropna()

# Extract feature matrix (X) and target variable (y)
X = data[features]
y = data[target]

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train Decision Tree Regressor
model = DecisionTreeRegressor()
model.fit(X_train, y_train)

# Make predictions
predictions = model.predict(X_test)

# Model Evaluation
mae = mean_absolute_error(y_test, predictions)
r2 = r2_score(y_test, predictions)

print(f"Mean Absolute Error: {mae:.2f}")
print(f"RÂ² Score: {r2:.2f}")
